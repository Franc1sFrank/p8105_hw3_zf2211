---
title: "P8105_hw3_zf2211"
author: "Francis"
date: "10/10/2018"
output: github_document
---
```{r}
library(tidyverse)
```

#Problem 1

```{r}
#read data
library(p8105.datasets)
data(brfss_smart2010, package = "p8105.datasets")
```

```{r}
#tidy up data
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response = as_factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE))
```

In 2002, states
```{r}
brfss %>% 
  filter(year == "2002") %>% 
  distinct(locationabbr, locationdesc) %>% 
  count(locationabbr) %>% 
  filter(n == 7)
```
were observed at 7 locations.


```{r}
#create a dataset for spaghetti plot
brfss1 = brfss %>% 
  group_by(locationabbr, year) %>% 
  distinct(locationdesc) %>% 
  count(locationabbr)
```

```{r}
#spaghetti plot
ggplot(brfss1, aes(x = year, y = n, color = locationabbr)) +
  geom_line() +
  geom_point()
```
This spaghetti plot shows the number of locations change in each state from 2002 to 2010. As we can see, most states have the similar trend and the change is relatively small, while FL had big fluctuation in 2007 and 2010.

```{r}
#filter data
brfss %>% 
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(locationabbr == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean = mean(data_value), 
            sd = sd(data_value)) %>% 
#make a table  
  knitr::kable(digits = 1)
```

From the table, the mean of NY "Excellent" response decreased from 24.0%(2002) to 22.5%(2006) and keeps similar to 22.7%(2010). The SD of NY "Excellent" response decreased from 4.5%(2002) to 4.0%(2006) to 3.6%(2010).

```{r}
#filter data
brfss_avg = brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(avg_prop = mean(data_value))
#plot histogram
ggplot(brfss_avg, aes(x = year, y = avg_prop, color = locationabbr)) +
  geom_line() +
  labs(x = "Year",
       y = "Average percentage of each response",
       title = "The state-level average percent of each response from 2002 to 2010") +
  facet_grid(. ~ response) + #seperate into 5 groups
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 0.8)) +
  guides(colour = guide_legend(nrow = 3)) #row of legend
```
As we can see, the percent of each response from high to low is "Very good", "Good", "Excellent", "Fair" and "Poor". But we could check this conclusion later if have spare time.


#Problem2

```{r}
#read data and print
data(instacart, package = "p8105.datasets")
print(instacart)
#summary
summary(instacart)
```
Instacart dataset contains `r nrow(instacart)` observations and `r ncol(instacart)` variables, showing numeric variables like order ID, order hours; and character value like product name and department. 
For example, the order_id = 1 which ordered by user_id = 112108, have 8 product which add_to_cart_order is product_id 49302, 11109, 10246, 49683, 43633, 13176, 47209, 22035. Among them, add_to_cart_order 1,2,5,8 are reordered. instacart$user_id = 112108 ordered 4 each product on Thursday at 10 p.m. after 9 days of last order. The aisle_id, aisle, department_id, department of each product are shown too.

There are 
```{r}
instacart %>% 
  distinct(aisle) %>% 
  nrow()
```
aisles.

```{r}
# number of items from each aisle
aisle_num <- 
instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  arrange(desc(n))
aisle_num
```
`fresh vegetables` and `fresh fruits` are the aisle where most item ordered from.

```{r}
#plot aisle
plot_aisle <- function(left, right){
  ggplot(filter(aisle_num, n >= left & n < right), aes(x = reorder(aisle, -n), y = n)) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = n), size = 2, position = position_dodge(width = 0.9), vjust = -0.25) +
  scale_y_continuous(name = "Number of items ordered") +
  scale_x_discrete(name = "Asile" ) +
  coord_cartesian(ylim = c(left * 0.95, right * 1.05)) +
  labs(title = paste("Aisles from which more than", left, "and less than", right, "items ordered")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
       axis.title = element_text(size = 10, face = "bold")
)
}
plot1 = plot_aisle (0, 1000)
plot2 = plot_aisle (1000, 2000)
plot3 = plot_aisle (2000, 6000)
plot4 = plot_aisle (6000, 13000)
plot5 = plot_aisle (13000, 151000)
plot5  
plot4  
plot3  
plot2 
plot1 
```



The 134 aisles were into 5 groups(0-1000, 1000-2000, 2000-6000, 6000-13000, 13000-151000), and plot each group a barchart in a descending way. Fresh vegetables and fresh fruits are outstanding high while other aisles are in a relatively smoothly descending way.

The most popular items in each aisle is:
```{r}
# make a table showing the most popular item in aisles "baking ingredients", "dog food care", "packaged vegetables fruits"
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(item_ranking = min_rank(desc(n))) %>% 
  filter(item_ranking == 1) %>% 
  select(-item_ranking) %>% 
  knitr::kable()
```

Table shows the most products in 3 aisles are "Light Brown Sugar", "Snack Sticks Chicken & Rice Recipe Dog Treats", and "Organic Baby Spinach" respectively.
Since aisle "package vegetables fruits" is much bigger than "dog food care", there are a lot more "Organic Baby Spinach" order than "Snack Stick Chichen & Rice Recipe Dog Treats" order in each aisle correspondingly.


```{r}
# filter product_name and make adjustments to date/time variables accordingly.
instacart %>% 
  filter(product_name %in%  c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hr = mean(order_hour_of_day)) %>% 
  mutate(mean_hr = paste(floor(mean_hr), round((mean_hr - floor(mean_hr)) * 60), sep=":")) %>% 
  spread(key = order_dow, value = mean_hr) %>% 
  rename('Sun' = '0', 'Mon'= '1', 'Tue' = '2', 'Wed' = '3', 'Thu' = '4', 'Fri' = '5', 'Sat' = '6') %>%
  knitr::kable() 
```



The table lists the average hour of day when Pink Lady Apples and Coffee Ice Cream are ordered. We can say the two items are almost ordered evenly in the day, because the mean of the time are between 11:30 and 15:00. The Coffee Ice Cream is ordered more in the afternoon since its mean time is later than Pink Lady Apples.


#Problem3

```{r}
#read data
data(ny_noaa, package = "p8105.datasets")
ny_noaa
```

The national daily weather dataset `ny_noaa` dataset contains `r nrow(ny_noaa)` records and `r ncol(ny_noaa)` variables. The records are reported from `r nrow(distinct(ny_noaa,id))` distinct stations, from `r range(ny_noaa$date)[1]` to `r range(ny_noaa$date)[2]`. There are precipitation(tenths of mm), snowfall(mm), snow depth(mm) and maximun / minimum temperature(tenths of ºC) in this dataset. There are `r round(sum(is.na(ny_noaa$tmax)) / nrow(ny_noaa)*100)` % missing values in the variable `tmax` and `tmin`; `r round(sum(is.na(ny_noaa$snow)) / nrow(ny_noaa)*100)`% in `snow`; `r round(sum(is.na(ny_noaa$snwd)) / nrow(ny_noaa)*100)`% in `snwd`; `r round(sum(is.na(ny_noaa$prcp)) / nrow(ny_noaa)*100)`% in `prcp`. There are several percents data loss in different variables, may influence the reliability of analysis.
  


Data cleaning
```{r}
#Create year, month, day variables
ny_noaa <- 
  ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-")
```


The most commonly observed value of snowfall is 
```{r}
# change variable types
ny_noaa$tmax = as.numeric(unclass(ny_noaa$tmax))
ny_noaa$tmin = as.numeric(unclass(ny_noaa$tmin))
ny_noaa$year = as.numeric(unclass(ny_noaa$year))
ny_noaa$month = as.numeric(unclass(ny_noaa$month))
ny_noaa$day = as.numeric(unclass(ny_noaa$day))

#mutate the value of varialbes so that they are in integral units (mm and ºC)
ny_noaa <- 
  ny_noaa %>% 
  mutate(tmax = tmax * 0.1, tmin = tmin * 0.1, prcp = prcp * 0.1)

#mode of snowfall
getmode <- function(input) {
   uniqv <- unique(input)
   uniqv[which.max(tabulate(match(input, uniqv)))]
}
getmode(ny_noaa$snow)
```
mm. Maybe that's because the snowless day in NY is more than snow day.


Average max temp. in January and July:
```{r}
library(gridExtra)
#avg temp in Jan
avg_jan <- 
  ny_noaa %>% 
  na.omit("tmax") %>% 
  filter(month == 1) %>% 
  group_by(id, year) %>% 
  summarize(avg_temp = mean(tmax)) 
p_jan  <-  
ggplot (avg_jan, aes(x = year, y = avg_temp, color = id)) +
  geom_line() +
  labs (title = "Average Temperature (ºC) in January, 1980-2010",
        x = "Year",
        y = "Temperature (ºC)") +
  theme(legend.position="none")

# avg temp in Jul
avg_jul <- 
  ny_noaa %>% 
  na.omit("tmax") %>% 
  filter(month == 7) %>% 
  group_by(id, year) %>% 
  summarize(avg_temp = mean(tmax) ) 
p_jul <-  
ggplot (avg_jul, aes(x = year, y = avg_temp, color = id)) +
  geom_line() +
  labs (title = "Average Temperature (ºC) in July, 1980-2010",
        x = "Year",
        y = "Temperature (ºC)") +
  theme(legend.position="none")
#two-panel plot
grid.arrange(p_jan, p_jul, nrow = 2)
```
```{r}
#find outliers
filter(avg_jan, avg_temp == min(avg_jan$avg_temp))
filter(avg_jan, avg_temp == max(avg_jan$avg_temp))
filter(avg_jul, avg_temp == min(avg_jul$avg_temp))
filter(avg_jul, avg_temp == max(avg_jul$avg_temp))
```

The average temperature in January/July, 1980 to 2010 is relatively similar and smooth. As two most orthodox points, Jan 1982(-16.65ºC) and Jul 1988(13.95ºC) have lowest average max temperature while Jan 1997(13.3ºC) and Jul 2010(33.6ºC) have highest average max temperature.


```{r fig.height = 10, fig.width = 10}
# plotting tmax vs tmin using geom_hex
hex <- 
  ny_noaa %>% 
  na.omit(cols= c("tmax", "tmin")) %>%  #omit NA
  ggplot(aes(x = tmin, y = tmax)) +
  labs(title = "Max Temperature vs Min Temperature",
       x = "Min Temperature (ºC)",
       y = "Max Temperature (ºC)") +
  geom_hex()
```


```{r fig.height = 10, fig.width = 10}
# plot distribution of snowfall values with geom_density_ridges
library(ggridges)
density <-  
  ny_noaa  %>% 
  na.omit(cols = "snow_mm") %>% #omit NA
  filter(snow > 0 & snow <100) %>% 
  ggplot(aes(x = snow, y = as.character(year))) +
  geom_density_ridges(adjust = 3) +
  labs (title = "Snowfall Distribution(0-100mm) from 1980 to 2010",
        x = "Snowfall (mm)",
        y = "Year") 
# make a two panel plot
grid.arrange(hex, density, nrow = 2)
```

From the plot above, the Max and Min temperatures seems have positive-relationship. And the Max temperature has a dense center around 0-25ºC and Min temperature around -5-20ºC.
  
From the plot below, during these years, the yearly snowfall distribution is similar, have 3 dense ranges around 0-30mm, 45-55m and 75mm.

